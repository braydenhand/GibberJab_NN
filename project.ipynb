{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Data over Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message: 'Brayden Hand is my name and I am testing the output of ggwave'\n",
      "Waveform length: 868352\n",
      "Playing audio...\n",
      "Transmission complete\n"
     ]
    }
   ],
   "source": [
    "import ggwave\n",
    "import pyaudio\n",
    "import numpy as np\n",
    " \n",
    "def send_message(message=\"hello\"):\n",
    "    print(f\"Sending message: '{message}'\")\n",
    "\n",
    "    # Init ggwave\n",
    "    instance = ggwave.init()\n",
    "\n",
    "    try:\n",
    "        waveform = ggwave.encode(message)\n",
    "        print(\"Waveform length:\", len(waveform))\n",
    "\n",
    "        # Interpret waveform as float32\n",
    "        audio_data = np.frombuffer(waveform, dtype=np.float32)\n",
    "\n",
    "        # Play it using PyAudio\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=pyaudio.paFloat32,\n",
    "                        channels=1,\n",
    "                        rate=48000,\n",
    "                        output=True)\n",
    "\n",
    "        print(\"Playing audio...\")\n",
    "        stream.write(audio_data.tobytes())\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        print(\"Transmission complete\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        ggwave.free(instance)\n",
    "\n",
    "send_message(\"Brayden Hand is my name and I am testing the output of ggwave\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive Data over Audio (Listener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ggwave\n",
    "import pyaudio\n",
    "import time\n",
    "\n",
    "def listen_for_messages(duration=60):\n",
    "    \"\"\"\n",
    "    Listen for GGWave messages for the specified duration (in seconds)\n",
    "    using PyAudio instead of SoX.\n",
    "    \"\"\"\n",
    "    print(f\"Listening for {duration} seconds...\")\n",
    "\n",
    "    # GGWave expects:\n",
    "    SAMPLE_RATE = 48000\n",
    "    SAMPLE_FORMAT = pyaudio.paFloat32  # 32-bit float\n",
    "    CHANNELS = 1\n",
    "    CHUNK = 1024  # Buffer size per read (tweak for latency vs CPU)\n",
    "\n",
    "    # Initialize GGWave and PyAudio\n",
    "    ggwave_instance = ggwave.init()\n",
    "    received_messages = set()\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    stream = audio.open(\n",
    "        format=SAMPLE_FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=SAMPLE_RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(\"Listening... Press Ctrl+C to stop early.\")\n",
    "        while time.time() - start_time < duration:\n",
    "            # Read a chunk of raw audio data\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "\n",
    "            # Decode using GGWave\n",
    "            result = ggwave.decode(ggwave_instance, data)\n",
    "            if result:\n",
    "                try:\n",
    "                    message = result.decode(\"utf-8\")\n",
    "                    if message not in received_messages:\n",
    "                        print(f\"Received: \\\"{message}\\\"\")\n",
    "                        received_messages.add(message)\n",
    "                except UnicodeDecodeError:\n",
    "                    # Occasionally garbled noise might sneak in\n",
    "                    continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped listening.\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "        ggwave.free(ggwave_instance)\n",
    "\n",
    "        if not received_messages:\n",
    "            print(\"No messages received.\")\n",
    "        else:\n",
    "            print(f\"Listening complete. Received {len(received_messages)} unique messages.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing compressor...\n",
      "Message split into 4 chunks based on byte size limit of 125 bytes\n",
      "\n",
      "Sending chunk 1/4 (48 bytes):\n",
      "Original message: '[1/4] In an age where technology evolves at a breakneck '\n",
      "Compressed message: '\\x00[1/4] \\x12\\x15\\x0cgeÁôtechnolog\\x95evolvesŜa§reakneck '\n",
      "Original: 56 bytes, Compressed: 48 bytes\n",
      "Compression ratio: 1.17x\n",
      "Transmitting...\n",
      "Transmission complete\n",
      "\n",
      "Sending chunk 2/4 (86 bytes):\n",
      "Original message: '[2/4] pace, the integration of artificial intelligence into daily life has become not just a possibility, '\n",
      "Compressed message: '\\x00[2/4]Ĕacƽ\\x01ɘegration\\x1bartificialɘelligencȝ\\x85daičlifƑs±comeõ just©possibilitȒ'\n",
      "Original: 106 bytes, Compressed: 86 bytes\n",
      "Compression ratio: 1.23x\n",
      "Transmitting...\n",
      "Transmission complete\n",
      "\n",
      "Sending chunk 3/4 (86 bytes):\n",
      "Original message: '[3/4] but an inevitability that touches nearly every industry and facet of human interaction, transforming'\n",
      "Compressed message: '\\x00[3/4]ƿ\\x15\\x98evitabilit\\x95\\x82\\x80uchųnearčevery\\x98dustȣ\\x0efacet\\x1bhumanɘeractionƚransforming'\n",
      "Original: 106 bytes, Compressed: 86 bytes\n",
      "Compression ratio: 1.23x\n",
      "Transmitting...\n",
      "Transmission complete\n",
      "\n",
      "Sending chunk 4/4 (57 bytes):\n",
      "Original message: '[4/4]  the very way people live, work, learn, and relate to one another.'\n",
      "Compressed message: '\\x00[4/4] \\x01 very°\\x95peopůlivówork\\x1clearȖ\\x0erelate\\x16Ǯanother.'\n",
      "Original: 72 bytes, Compressed: 57 bytes\n",
      "Compression ratio: 1.26x\n",
      "Transmitting...\n",
      "Transmission complete\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import ggwave\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import wave\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Add TextCompressor class definition\n",
    "class TextCompressor:\n",
    "    \"\"\"\n",
    "    A text compression system that uses frequency analysis of words, syllables, \n",
    "    and character patterns to create an optimized encoding dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_path=None):\n",
    "        \"\"\"Initialize the compressor with an optional pre-trained dictionary\"\"\"\n",
    "        if load_path and os.path.exists(load_path):\n",
    "            self.load_dictionary(load_path)\n",
    "        else:\n",
    "            # Dictionary mapping patterns to their encoded representations\n",
    "            self.encoding_dict = {}\n",
    "            # Reverse mapping for decoding\n",
    "            self.decoding_dict = {}\n",
    "            # Reserve first 32 characters (non-printable ASCII) as encoding markers\n",
    "            self.next_code = 1\n",
    "            # Special marker to identify compressed content\n",
    "            self.COMPRESSION_MARKER = chr(0)\n",
    "    \n",
    "    def train(self, corpus, min_frequency=5, max_patterns=256):\n",
    "        \"\"\"\n",
    "        Train the compressor on a text corpus to identify common patterns\n",
    "        \n",
    "        Args:\n",
    "            corpus: Text to analyze for patterns\n",
    "            min_frequency: Minimum frequency for a pattern to be included\n",
    "            max_patterns: Maximum number of patterns to encode\n",
    "        \"\"\"\n",
    "        # Tokenize the corpus into words and analyze frequency\n",
    "        words = re.findall(r'\\w+', corpus.lower())\n",
    "        word_freq = collections.Counter(words)\n",
    "        \n",
    "        # Find common character sequences (n-grams)\n",
    "        ngrams = []\n",
    "        for n in range(2, 5):  # 2-grams, 3-grams, 4-grams\n",
    "            for i in range(len(corpus) - n + 1):\n",
    "                ngrams.append(corpus[i:i+n])\n",
    "        ngram_freq = collections.Counter(ngrams)\n",
    "        \n",
    "        # Combine word and ngram frequencies\n",
    "        combined_freq = {}\n",
    "        for item, freq in word_freq.items():\n",
    "            if len(item) > 2 and freq >= min_frequency:  # Only include words longer than 2 chars\n",
    "                # Calculate space saving: frequency * (len + 1 for space - 1 for code)\n",
    "                saving = freq * len(item)\n",
    "                combined_freq[item] = (freq, saving)\n",
    "                \n",
    "        for item, freq in ngram_freq.items():\n",
    "            if freq >= min_frequency and item not in combined_freq:\n",
    "                saving = freq * (len(item) - 1)\n",
    "                combined_freq[item] = (freq, saving)\n",
    "        \n",
    "        # Sort by space saving potential\n",
    "        sorted_patterns = sorted(combined_freq.items(), key=lambda x: x[1][1], reverse=True)\n",
    "        \n",
    "        # Take top patterns, limited by max_patterns\n",
    "        top_patterns = sorted_patterns[:max_patterns]\n",
    "        \n",
    "        # Create encoding dictionary using control characters (1-31) and extended ASCII\n",
    "        self.encoding_dict = {}\n",
    "        self.decoding_dict = {}\n",
    "        \n",
    "        for i, (pattern, _) in enumerate(top_patterns):\n",
    "            # Start at 1 to avoid NULL character (0), use extended ASCII after 31\n",
    "            code = chr(1 + i) if i < 31 else chr(128 + (i - 31))\n",
    "            self.encoding_dict[pattern] = code\n",
    "            self.decoding_dict[code] = pattern\n",
    "        \n",
    "        # Always include the compression marker\n",
    "        self.COMPRESSION_MARKER = chr(0)\n",
    "        \n",
    "        return self.encoding_dict\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Compress text using the trained dictionary\n",
    "        \n",
    "        Args:\n",
    "            text: The text to compress\n",
    "            \n",
    "        Returns:\n",
    "            Compressed text with the compression marker prefix\n",
    "        \"\"\"\n",
    "        if not self.encoding_dict:\n",
    "            return text  # Can't compress without a dictionary\n",
    "        \n",
    "        compressed = text\n",
    "        \n",
    "        # Sort patterns by length (longest first) to avoid partial matches\n",
    "        patterns = sorted(self.encoding_dict.keys(), key=len, reverse=True)\n",
    "        \n",
    "        # Replace each pattern with its code\n",
    "        for pattern in patterns:\n",
    "            code = self.encoding_dict[pattern]\n",
    "            # For words, ensure we're replacing whole words\n",
    "            if re.match(r'^\\w+$', pattern):\n",
    "                compressed = re.sub(r'\\b' + re.escape(pattern) + r'\\b', code, compressed, flags=re.IGNORECASE)\n",
    "            else:\n",
    "                compressed = compressed.replace(pattern, code)\n",
    "        \n",
    "        # Add compression marker to indicate this is compressed\n",
    "        return self.COMPRESSION_MARKER + compressed\n",
    "    \n",
    "    def decode(self, compressed_text):\n",
    "        \"\"\"\n",
    "        Decompress text using the dictionary\n",
    "        \n",
    "        Args:\n",
    "            compressed_text: Compressed text starting with the compression marker\n",
    "            \n",
    "        Returns:\n",
    "            Decompressed text or original if not compressed\n",
    "        \"\"\"\n",
    "        # Check if this is actually compressed\n",
    "        if not compressed_text or compressed_text[0] != self.COMPRESSION_MARKER:\n",
    "            return compressed_text\n",
    "        \n",
    "        # Remove the compression marker\n",
    "        text = compressed_text[1:]\n",
    "        \n",
    "        # Replace each code with its pattern\n",
    "        for code, pattern in self.decoding_dict.items():\n",
    "            text = text.replace(code, pattern)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save_dictionary(self, path):\n",
    "        \"\"\"Save the encoding dictionary to a file\"\"\"\n",
    "        data = {\n",
    "            'encoding_dict': self.encoding_dict,\n",
    "            'decoding_dict': self.decoding_dict,\n",
    "            'compression_marker': ord(self.COMPRESSION_MARKER)\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    def load_dictionary(self, path):\n",
    "        \"\"\"Load the encoding dictionary from a file\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        self.encoding_dict = data['encoding_dict']\n",
    "        self.decoding_dict = data['decoding_dict']\n",
    "        self.COMPRESSION_MARKER = chr(data['compression_marker'])\n",
    "    \n",
    "    def compression_stats(self, original, compressed):\n",
    "        \"\"\"Calculate compression statistics\"\"\"\n",
    "        original_size = len(original.encode('utf-8'))\n",
    "        compressed_size = len(compressed.encode('utf-8'))\n",
    "        ratio = original_size / compressed_size if compressed_size > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'original_size': original_size,\n",
    "            'compressed_size': compressed_size,\n",
    "            'compression_ratio': ratio,\n",
    "            'space_saving': (1 - 1/ratio) * 100 if ratio > 0 else 0\n",
    "        }\n",
    "\n",
    "def create_deep_corpus(output_file=\"deep_corpus.txt\", corpus_size=10000000):\n",
    "    \"\"\"\n",
    "    Create a deep English corpus by downloading classic literature texts\n",
    "    from Project Gutenberg. This creates a rich training set for compression.\n",
    "    \n",
    "    Args:\n",
    "        output_file: File to save the corpus to\n",
    "        corpus_size: Approximate size of corpus in characters\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created corpus file\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of classic literature texts from Project Gutenberg (public domain)\n",
    "    gutenberg_texts = [\n",
    "    # Classic Literature - Fiction\n",
    "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride and Prejudice by Jane Austen\n",
    "    \"https://www.gutenberg.org/files/84/84-0.txt\",      # Frankenstein by Mary Shelley\n",
    "    \"https://www.gutenberg.org/files/98/98-0.txt\",      # A Tale of Two Cities by Charles Dickens\n",
    "    \"https://www.gutenberg.org/files/2701/2701-0.txt\",  # Moby Dick by Herman Melville\n",
    "    \"https://www.gutenberg.org/files/345/345-0.txt\",    # Dracula by Bram Stoker\n",
    "    \"https://www.gutenberg.org/files/76/76-0.txt\",      # Adventures of Huckleberry Finn by Mark Twain\n",
    "    \"https://www.gutenberg.org/files/1661/1661-0.txt\",  # Sherlock Holmes by Arthur Conan Doyle\n",
    "    \"https://www.gutenberg.org/files/174/174-0.txt\",    # The Picture of Dorian Gray by Oscar Wilde\n",
    "    \"https://www.gutenberg.org/files/145/145-0.txt\",    # Middlemarch by George Eliot\n",
    "    \n",
    "    # Classic Literature - Additional Genres\n",
    "    \"https://www.gutenberg.org/files/2641/2641-0.txt\",  # A Room with a View by E.M. Forster\n",
    "    \"https://www.gutenberg.org/files/11/11-0.txt\",      # Alice's Adventures in Wonderland by Lewis Carroll\n",
    "    \"https://www.gutenberg.org/files/1400/1400-0.txt\",  # Great Expectations by Charles Dickens\n",
    "    \"https://www.gutenberg.org/files/768/768-0.txt\",    # Wuthering Heights by Emily Brontë\n",
    "    \"https://www.gutenberg.org/files/2814/2814-0.txt\",  # Dubliners by James Joyce\n",
    "    ]\n",
    "    \n",
    "    # Create corpus by downloading and concatenating texts\n",
    "    corpus_text = \"\"\n",
    "    texts_used = 0\n",
    "    \n",
    "    for url in gutenberg_texts:\n",
    "        try:\n",
    "            print(f\"Downloading {url}\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                # Extract the text, removing Project Gutenberg header/footer\n",
    "                text = response.text\n",
    "                \n",
    "                # Remove Gutenberg header (everything before the first empty line after \"START\")\n",
    "                start_marker = \"*** START OF\"\n",
    "                if start_marker in text:\n",
    "                    start_pos = text.find(start_marker)\n",
    "                    text = text[start_pos:]\n",
    "                    # Find the first paragraph break after the marker\n",
    "                    para_break = text.find(\"\\r\\n\\r\\n\")\n",
    "                    if para_break > 0:\n",
    "                        text = text[para_break+4:]\n",
    "                \n",
    "                # Remove Gutenberg footer (everything after \"END\")\n",
    "                end_marker = \"*** END OF\"\n",
    "                if end_marker in text:\n",
    "                    end_pos = text.find(end_marker)\n",
    "                    text = text[:end_pos]\n",
    "                \n",
    "                # Add to corpus\n",
    "                corpus_text += text + \"\\n\\n\"\n",
    "                texts_used += 1\n",
    "                \n",
    "                # Check if we've reached the target size\n",
    "                if len(corpus_text) >= corpus_size:\n",
    "                    break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "    \n",
    "    # Save corpus to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(corpus_text)\n",
    "    \n",
    "    print(f\"Created corpus with {len(corpus_text)} characters from {texts_used} texts\")\n",
    "    print(f\"Saved to {output_file}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def train_deep_compressor(corpus_path=None, output_path=\"deep_compressor.pkl\", max_patterns=512):\n",
    "    \"\"\"\n",
    "    Train a TextCompressor on a deep English corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus_path: Path to corpus file (will create one if None)\n",
    "        output_path: Where to save the trained compressor\n",
    "        max_patterns: Maximum number of patterns to include in the dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Trained TextCompressor instance\n",
    "    \"\"\"\n",
    "    # Create corpus if needed\n",
    "    if not corpus_path:\n",
    "        corpus_path = create_deep_corpus()\n",
    "    \n",
    "    print(f\"Training compressor on {corpus_path} with max_patterns={max_patterns}\")\n",
    "    \n",
    "    # Load corpus\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        corpus = f.read()\n",
    "    \n",
    "    # Create and train compressor\n",
    "    compressor = TextCompressor()\n",
    "    \n",
    "    # Train with a lower minimum frequency for more patterns but higher max_patterns\n",
    "    start_time = time.time()\n",
    "    compressor.train(corpus, min_frequency=3, max_patterns=max_patterns)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.1f} seconds\")\n",
    "    print(f\"Trained dictionary has {len(compressor.encoding_dict)} patterns\")\n",
    "    \n",
    "    # Save the trained compressor\n",
    "    compressor.save_dictionary(output_path)\n",
    "    print(f\"Saved compressor to {output_path}\")\n",
    "    \n",
    "    return compressor\n",
    "\n",
    "def send_encoded_message(message, compressor=None, protocol_id=6, max_bytes=125):\n",
    "    \"\"\"\n",
    "    Compress and send a message using ggwave with byte-size-aware chunking\n",
    "    \n",
    "    Args:\n",
    "        message: Text message to compress and send\n",
    "        compressor: Optional TextCompressor instance (will create or load one if None)\n",
    "        protocol_id: GGWave protocol ID (higher = better for longer messages)\n",
    "        max_bytes: Maximum size for a single transmission chunk in bytes\n",
    "    \"\"\"\n",
    "    # Use provided compressor or load a pre-trained one\n",
    "    if compressor is None:\n",
    "        if os.path.exists(\"deep_compressor.pkl\"):\n",
    "            compressor = TextCompressor(\"deep_compressor.pkl\")\n",
    "        else:\n",
    "            compressor = train_deep_compressor()\n",
    "    \n",
    "    # Initial character-based chunk size estimate (start conservative)\n",
    "    # We'll adjust this based on actual compressed size\n",
    "    estimated_chars_per_chunk = 50  # Start with a conservative estimate\n",
    "    \n",
    "    # Split message into chunks, ensuring each compressed chunk is under max_bytes\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(message):\n",
    "        # Start with the estimated chunk size\n",
    "        end = min(start + estimated_chars_per_chunk, len(message))\n",
    "        \n",
    "        # Get potential chunk\n",
    "        potential_chunk = message[start:end]\n",
    "        \n",
    "        # Test compress it\n",
    "        chunk_header = f\"[0/0] \"  # Temporary header\n",
    "        test_chunk = chunk_header + potential_chunk\n",
    "        compressed = compressor.encode(test_chunk)\n",
    "        \n",
    "        # Check compressed size in bytes\n",
    "        compressed_size = len(compressed.encode('utf-8'))\n",
    "        \n",
    "        # Binary search to find optimal chunk size\n",
    "        # If too big, reduce; if small enough, try adding more\n",
    "        min_size = 1\n",
    "        max_size = end - start\n",
    "        \n",
    "        while min_size < max_size:\n",
    "            if compressed_size > max_bytes:\n",
    "                # Too big, reduce size\n",
    "                max_size = (min_size + max_size) // 2\n",
    "                potential_chunk = message[start:start + max_size]\n",
    "                test_chunk = chunk_header + potential_chunk\n",
    "                compressed = compressor.encode(test_chunk)\n",
    "                compressed_size = len(compressed.encode('utf-8'))\n",
    "            else:\n",
    "                # Small enough, try increasing\n",
    "                old_min = min_size\n",
    "                min_size = min_size + (max_size - min_size) // 2\n",
    "                if min_size == old_min:\n",
    "                    min_size = max_size  # Break if no progress\n",
    "                \n",
    "                if start + min_size >= len(message):\n",
    "                    # Reached end of message\n",
    "                    potential_chunk = message[start:]\n",
    "                    break\n",
    "                \n",
    "                larger_chunk = message[start:start + min_size]\n",
    "                test_chunk = chunk_header + larger_chunk\n",
    "                compressed = compressor.encode(test_chunk)\n",
    "                compressed_size = len(compressed.encode('utf-8'))\n",
    "                \n",
    "                if compressed_size <= max_bytes:\n",
    "                    potential_chunk = larger_chunk  # Update if still under limit\n",
    "        \n",
    "        # Add this chunk and move to next section\n",
    "        chunks.append(potential_chunk)\n",
    "        start += len(potential_chunk)\n",
    "        \n",
    "        # Update the estimate for next iteration based on compressed ratio\n",
    "        content_bytes = len(potential_chunk.encode('utf-8'))\n",
    "        if content_bytes > 0:\n",
    "            # Calculate actual compressed ratio for this chunk\n",
    "            compressed_ratio = compressed_size / content_bytes\n",
    "            # Estimate for next chunk\n",
    "            estimated_chars_per_chunk = int((max_bytes / compressed_ratio) * 0.9)  # 10% safety margin\n",
    "            # Keep it in a reasonable range\n",
    "            estimated_chars_per_chunk = max(10, min(estimated_chars_per_chunk, 100))\n",
    "    \n",
    "    print(f\"Message split into {len(chunks)} chunks based on byte size limit of {max_bytes} bytes\")\n",
    "    \n",
    "    # Send each chunk with properly numbered headers\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_header = f\"[{i+1}/{len(chunks)}] \"\n",
    "        full_chunk = chunk_header + chunk\n",
    "        \n",
    "        # Double-check size before sending\n",
    "        compressed = compressor.encode(full_chunk)\n",
    "        compressed_size = len(compressed.encode('utf-8'))\n",
    "        \n",
    "        print(f\"\\nSending chunk {i+1}/{len(chunks)} ({compressed_size} bytes):\")\n",
    "        \n",
    "        if compressed_size > max_bytes:\n",
    "            print(f\"WARNING: Chunk size {compressed_size} bytes exceeds target {max_bytes} bytes\")\n",
    "            print(\"This might cause transmission issues\")\n",
    "        \n",
    "        # Send this chunk\n",
    "        _send_single_chunk(full_chunk, compressor, protocol_id)\n",
    "        \n",
    "        # Wait between chunks to avoid overlap\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def _send_single_chunk(message, compressor, protocol_id=6):\n",
    "    \"\"\"Helper function to compress and send a single message chunk using PyAudio\"\"\"\n",
    "    # Compress the message\n",
    "    original_message = message\n",
    "    compressed_message = compressor.encode(message)\n",
    "    \n",
    "    # Print only the required information\n",
    "    stats = compressor.compression_stats(original_message, compressed_message)\n",
    "    print(f\"Original message: '{original_message}'\")\n",
    "    print(f\"Compressed message: {repr(compressed_message)}\")\n",
    "    print(f\"Original: {stats['original_size']} bytes, Compressed: {stats['compressed_size']} bytes\")\n",
    "    print(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize ggwave\n",
    "        instance = ggwave.init()\n",
    "        \n",
    "        # Encode the message with ggwave using the specified protocol\n",
    "        # protocol_id controls type of transmission (higher values usually support longer messages)\n",
    "        waveform = ggwave.encode(compressed_message, instance, protocol_id)\n",
    "        \n",
    "        # Convert waveform to numpy array for PyAudio\n",
    "        # The waveform from ggwave is a bytes object containing 32-bit float samples\n",
    "        audio_data = np.frombuffer(waveform, dtype=np.float32)\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        # Open a stream for playback\n",
    "        stream = p.open(\n",
    "            format=pyaudio.paFloat32,\n",
    "            channels=1,\n",
    "            rate=48000,\n",
    "            output=True\n",
    "        )\n",
    "        \n",
    "        # Play the audio data\n",
    "        print(\"Transmitting...\")\n",
    "        stream.write(audio_data.tobytes())\n",
    "        \n",
    "        # Close the audio stream and PyAudio\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        \n",
    "        print(\"Transmission complete\")\n",
    "        \n",
    "        # Free ggwave\n",
    "        ggwave.free(instance)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return compressed_message\n",
    "\n",
    "def listen_for_deep_encoded_messages(duration=60, protocol_id=6):\n",
    "    \"\"\"\n",
    "    Listen for deeply encoded messages using the trained compressor with PyAudio\n",
    "    \n",
    "    Args:\n",
    "        duration: How long to listen in seconds\n",
    "        protocol_id: GGWave protocol ID to listen for (should match sender)\n",
    "    \"\"\"\n",
    "    # Load the deep compressor if available\n",
    "    if os.path.exists(\"deep_compressor.pkl\"):\n",
    "        compressor = TextCompressor(\"deep_compressor.pkl\")\n",
    "    else:\n",
    "        compressor = train_deep_compressor()\n",
    "    \n",
    "    # Create an audio data queue for communication between threads\n",
    "    audio_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "    \n",
    "    # Initialize ggwave\n",
    "    instance = ggwave.init()\n",
    "    \n",
    "    # Track received messages and message chunks\n",
    "    received_messages = set()\n",
    "    chunk_buffer = {}  # Format: {msg_id: {total_chunks: n, chunks: {1: \"text\", 2: \"more text\"}}}\n",
    "    \n",
    "    # PyAudio callback function to process audio data\n",
    "    def audio_callback(in_data, frame_count, time_info, status):\n",
    "        if not stop_event.is_set():\n",
    "            # Add the audio data to the queue\n",
    "            audio_queue.put(in_data)\n",
    "        return (None, pyaudio.paContinue)\n",
    "    \n",
    "    # Function to process audio data in a separate thread\n",
    "    def process_audio():\n",
    "        while not stop_event.is_set() or not audio_queue.empty():\n",
    "            try:\n",
    "                # Get audio data from the queue with a timeout\n",
    "                in_data = audio_queue.get(timeout=0.1)\n",
    "                \n",
    "                # Try to decode with ggwave\n",
    "                result = ggwave.decode(instance, in_data, protocol_id)\n",
    "                \n",
    "                if result is not None:\n",
    "                    message = result.decode('utf-8')\n",
    "                    \n",
    "                    # Check if compressed\n",
    "                    is_compressed = message and message[0] == compressor.COMPRESSION_MARKER\n",
    "                    \n",
    "                    if is_compressed:\n",
    "                        original = message\n",
    "                        decoded_message = compressor.decode(message)\n",
    "                        stats = compressor.compression_stats(decoded_message, original)\n",
    "                        \n",
    "                        # Check if this is a chunked message\n",
    "                        chunk_match = re.match(r'\\[(\\d+)/(\\d+)\\] (.*)', decoded_message)\n",
    "                        \n",
    "                        if chunk_match:\n",
    "                            # This is a chunked message\n",
    "                            chunk_num = int(chunk_match.group(1))\n",
    "                            total_chunks = int(chunk_match.group(2))\n",
    "                            chunk_content = chunk_match.group(3)\n",
    "                            \n",
    "                            # Create a unique ID for this message based on total chunks\n",
    "                            msg_id = f\"chunked_msg_{total_chunks}_{time.time():.0f}\"\n",
    "                            \n",
    "                            # Store or update chunk info\n",
    "                            if msg_id not in chunk_buffer:\n",
    "                                chunk_buffer[msg_id] = {\"total_chunks\": total_chunks, \"chunks\": {}}\n",
    "                            \n",
    "                            # Add this chunk\n",
    "                            chunk_buffer[msg_id][\"chunks\"][chunk_num] = chunk_content\n",
    "                            \n",
    "                            print(f\"Received chunk {chunk_num}/{total_chunks}\")\n",
    "                            \n",
    "                            # Check if we have all chunks for this message\n",
    "                            if len(chunk_buffer[msg_id][\"chunks\"]) == total_chunks:\n",
    "                                # Reconstruct the full message\n",
    "                                full_message = \"\"\n",
    "                                for i in range(1, total_chunks + 1):\n",
    "                                    full_message += chunk_buffer[msg_id][\"chunks\"][i]\n",
    "                                \n",
    "                                # Add to received messages\n",
    "                                if full_message not in received_messages:\n",
    "                                    print(f\"\\nReassembled chunked message:\")\n",
    "                                    print(f\"Original message: '{full_message}'\")\n",
    "                                    print(f\"Received in {total_chunks} chunks\")\n",
    "                                    received_messages.add(full_message)\n",
    "                        else:\n",
    "                            # Regular (non-chunked) message\n",
    "                            if decoded_message not in received_messages:\n",
    "                                print(f\"Original message: '{decoded_message}'\")\n",
    "                                print(f\"Compressed message: {repr(original)}\")\n",
    "                                print(f\"Original: {stats['original_size']} bytes, Compressed: {stats['compressed_size']} bytes\")\n",
    "                                print(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "                                print()\n",
    "                                received_messages.add(decoded_message)\n",
    "                    else:\n",
    "                        # Uncompressed message\n",
    "                        if message not in received_messages:\n",
    "                            print(f\"Received uncompressed message: {message}\")\n",
    "                            received_messages.add(message)\n",
    "            except queue.Empty:\n",
    "                # Queue is empty, continue\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing audio: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize PyAudio\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        # Open a stream for recording\n",
    "        stream = p.open(\n",
    "            format=pyaudio.paFloat32,\n",
    "            channels=1,\n",
    "            rate=48000,\n",
    "            input=True,\n",
    "            frames_per_buffer=4096,\n",
    "            stream_callback=audio_callback\n",
    "        )\n",
    "        \n",
    "        # Start the stream\n",
    "        stream.start_stream()\n",
    "        \n",
    "        print(f\"Listening for encoded messages for {duration} seconds...\")\n",
    "        \n",
    "        # Start the audio processing thread\n",
    "        process_thread = threading.Thread(target=process_audio)\n",
    "        process_thread.start()\n",
    "        \n",
    "        # Wait for the specified duration\n",
    "        time.sleep(duration)\n",
    "        \n",
    "        # Signal the thread to stop\n",
    "        stop_event.set()\n",
    "        \n",
    "        # Wait for the processing thread to finish\n",
    "        process_thread.join()\n",
    "        \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'stream' in locals() and stream.is_active():\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "        \n",
    "        if 'p' in locals():\n",
    "            p.terminate()\n",
    "        \n",
    "        ggwave.free(instance)\n",
    "    \n",
    "    return list(received_messages)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we already have a trained compressor\n",
    "        if os.path.exists(\"deep_compressor.pkl\"):\n",
    "            print(\"Loading existing compressor...\")\n",
    "            compressor = TextCompressor(\"deep_compressor.pkl\")\n",
    "        else:\n",
    "            # Create the corpus and train a deep compressor\n",
    "            print(\"Creating corpus and training compressor...\")\n",
    "            corpus_path = create_deep_corpus()\n",
    "            compressor = train_deep_compressor(corpus_path)\n",
    "        \n",
    "        # Example long message to send\n",
    "        example_message = \"\"\"In an age where technology evolves at a breakneck pace, the integration of artificial intelligence into daily life has become not just a possibility, but an inevitability that touches nearly every industry and facet of human interaction, transforming the very way people live, work, learn, and relate to one another.\"\"\"\n",
    "        \n",
    "        # Use the sender with:\n",
    "        # 1. Higher protocol_id (6) for longer message support\n",
    "        # 2. Smaller max_message_size (200) to ensure chunks are small enough\n",
    "        send_encoded_message(example_message, compressor, protocol_id=6, max_bytes=125)\n",
    "        \n",
    "        # To listen for messages, uncomment:\n",
    "        # listen_for_deep_encoded_messages(duration=60, protocol_id=6)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GibberJab_NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
