{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing compressor...\n",
      "Listening for encoded messages for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Receiving sound data ...\n",
      "Received end marker. Frames left = 1651, recorded = 155\n",
      "Analyzing captured data ..\n",
      "Decoded length = 48, protocol = 'Fast' (1)\n",
      "Received sound data successfully: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message: '[1/4] in an age where technology evolves at a breakneck '\n",
      "Compressed message: '\\x00[1/4] \\x10\\x13\\x0bge½ítechnolog\\x92evolvesŔa¤reakneck '\n",
      "Original: 56 bytes, Compressed: 48 bytes\n",
      "Compression ratio: 1.17x\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Receiving sound data ...\n",
      "Received end marker. Frames left = 1542, recorded = 264\n",
      "Analyzing captured data ..\n",
      "Decoded length = 86, protocol = 'Fast' (1)\n",
      "Received sound data successfully: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message: '[2/4] pace, the integration of artificial intelligence into daily life has become not just a possibility, '\n",
      "Compressed message: '\\x00[2/4]čacƴ\\x01Ɍegration\\x19artificialɌelligencȑ\\x82daiĆlifƈs®comeî just¦possibilitȆ'\n",
      "Original: 106 bytes, Compressed: 86 bytes\n",
      "Compression ratio: 1.23x\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Receiving sound data ...\n",
      "Received end marker. Frames left = 1548, recorded = 258\n",
      "Analyzing captured data ..\n",
      "Decoded length = 84, protocol = 'Fast' (1)\n",
      "Received sound data successfully: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message: '[3/4] but an inevitability that touches nearly every industry and facet of human interaction, transforming'\n",
      "Compressed message: '\\x00[3/4]ƶ\\x13\\x95evitabilit\\x92\\x1f\\x1duchūnearĆevery\\x95dustȗ\\rfacet\\x19humanɌeractionƑransforming'\n",
      "Original: 106 bytes, Compressed: 84 bytes\n",
      "Compression ratio: 1.26x\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Receiving sound data ...\n",
      "Received end marker. Frames left = 1622, recorded = 184\n",
      "Analyzing captured data ..\n",
      "Decoded length = 57, protocol = 'Fast' (1)\n",
      "Received sound data successfully: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message: '[4/4]  the very way people live, work, learn, and relate to one another.'\n",
      "Compressed message: '\\x00[4/4] \\x01 very\\xad\\x92peopŦlivìwork\\x1alearȊ\\rrelate\\x14Ǣanother.'\n",
      "Original: 72 bytes, Compressed: 57 bytes\n",
      "Compression ratio: 1.26x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import subprocess\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import ggwave  # Add explicit import for ggwave\n",
    "\n",
    "# Add TextCompressor class definition\n",
    "class TextCompressor:\n",
    "    \"\"\"\n",
    "    A text compression system that uses frequency analysis of words, syllables, \n",
    "    and character patterns to create an optimized encoding dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, load_path=None):\n",
    "        \"\"\"Initialize the compressor with an optional pre-trained dictionary\"\"\"\n",
    "        if load_path and os.path.exists(load_path):\n",
    "            self.load_dictionary(load_path)\n",
    "        else:\n",
    "            # Dictionary mapping patterns to their encoded representations\n",
    "            self.encoding_dict = {}\n",
    "            # Reverse mapping for decoding\n",
    "            self.decoding_dict = {}\n",
    "            # Reserve first 32 characters (non-printable ASCII) as encoding markers\n",
    "            self.next_code = 1\n",
    "            # Special marker to identify compressed content\n",
    "            self.COMPRESSION_MARKER = chr(0)\n",
    "    \n",
    "    def train(self, corpus, min_frequency=5, max_patterns=256):\n",
    "        \"\"\"\n",
    "        Train the compressor on a text corpus to identify common patterns\n",
    "        \n",
    "        Args:\n",
    "            corpus: Text to analyze for patterns\n",
    "            min_frequency: Minimum frequency for a pattern to be included\n",
    "            max_patterns: Maximum number of patterns to encode\n",
    "        \"\"\"\n",
    "        # Tokenize the corpus into words and analyze frequency\n",
    "        words = re.findall(r'\\w+', corpus.lower())\n",
    "        word_freq = collections.Counter(words)\n",
    "        \n",
    "        # Find common character sequences (n-grams)\n",
    "        ngrams = []\n",
    "        for n in range(2, 5):  # 2-grams, 3-grams, 4-grams\n",
    "            for i in range(len(corpus) - n + 1):\n",
    "                ngrams.append(corpus[i:i+n])\n",
    "        ngram_freq = collections.Counter(ngrams)\n",
    "        \n",
    "        # Combine word and ngram frequencies\n",
    "        combined_freq = {}\n",
    "        for item, freq in word_freq.items():\n",
    "            if len(item) > 2 and freq >= min_frequency:  # Only include words longer than 2 chars\n",
    "                # Calculate space saving: frequency * (len + 1 for space - 1 for code)\n",
    "                saving = freq * len(item)\n",
    "                combined_freq[item] = (freq, saving)\n",
    "                \n",
    "        for item, freq in ngram_freq.items():\n",
    "            if freq >= min_frequency and item not in combined_freq:\n",
    "                saving = freq * (len(item) - 1)\n",
    "                combined_freq[item] = (freq, saving)\n",
    "        \n",
    "        # Sort by space saving potential\n",
    "        sorted_patterns = sorted(combined_freq.items(), key=lambda x: x[1][1], reverse=True)\n",
    "        \n",
    "        # Take top patterns, limited by max_patterns\n",
    "        top_patterns = sorted_patterns[:max_patterns]\n",
    "        \n",
    "        # Create encoding dictionary using control characters (1-31) and extended ASCII\n",
    "        self.encoding_dict = {}\n",
    "        self.decoding_dict = {}\n",
    "        \n",
    "        for i, (pattern, _) in enumerate(top_patterns):\n",
    "            # Start at 1 to avoid NULL character (0), use extended ASCII after 31\n",
    "            code = chr(1 + i) if i < 31 else chr(128 + (i - 31))\n",
    "            self.encoding_dict[pattern] = code\n",
    "            self.decoding_dict[code] = pattern\n",
    "        \n",
    "        # Always include the compression marker\n",
    "        self.COMPRESSION_MARKER = chr(0)\n",
    "        \n",
    "        return self.encoding_dict\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Compress text using the trained dictionary\n",
    "        \n",
    "        Args:\n",
    "            text: The text to compress\n",
    "            \n",
    "        Returns:\n",
    "            Compressed text with the compression marker prefix\n",
    "        \"\"\"\n",
    "        if not self.encoding_dict:\n",
    "            return text  # Can't compress without a dictionary\n",
    "        \n",
    "        compressed = text\n",
    "        \n",
    "        # Sort patterns by length (longest first) to avoid partial matches\n",
    "        patterns = sorted(self.encoding_dict.keys(), key=len, reverse=True)\n",
    "        \n",
    "        # Replace each pattern with its code\n",
    "        for pattern in patterns:\n",
    "            code = self.encoding_dict[pattern]\n",
    "            # For words, ensure we're replacing whole words\n",
    "            if re.match(r'^\\w+$', pattern):\n",
    "                compressed = re.sub(r'\\b' + re.escape(pattern) + r'\\b', code, compressed, flags=re.IGNORECASE)\n",
    "            else:\n",
    "                compressed = compressed.replace(pattern, code)\n",
    "        \n",
    "        # Add compression marker to indicate this is compressed\n",
    "        return self.COMPRESSION_MARKER + compressed\n",
    "    \n",
    "    def decode(self, compressed_text):\n",
    "        \"\"\"\n",
    "        Decompress text using the dictionary\n",
    "        \n",
    "        Args:\n",
    "            compressed_text: Compressed text starting with the compression marker\n",
    "            \n",
    "        Returns:\n",
    "            Decompressed text or original if not compressed\n",
    "        \"\"\"\n",
    "        # Check if this is actually compressed\n",
    "        if not compressed_text or compressed_text[0] != self.COMPRESSION_MARKER:\n",
    "            return compressed_text\n",
    "        \n",
    "        # Remove the compression marker\n",
    "        text = compressed_text[1:]\n",
    "        \n",
    "        # Replace each code with its pattern\n",
    "        for code, pattern in self.decoding_dict.items():\n",
    "            text = text.replace(code, pattern)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save_dictionary(self, path):\n",
    "        \"\"\"Save the encoding dictionary to a file\"\"\"\n",
    "        data = {\n",
    "            'encoding_dict': self.encoding_dict,\n",
    "            'decoding_dict': self.decoding_dict,\n",
    "            'compression_marker': ord(self.COMPRESSION_MARKER)\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    def load_dictionary(self, path):\n",
    "        \"\"\"Load the encoding dictionary from a file\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        self.encoding_dict = data['encoding_dict']\n",
    "        self.decoding_dict = data['decoding_dict']\n",
    "        self.COMPRESSION_MARKER = chr(data['compression_marker'])\n",
    "    \n",
    "    def compression_stats(self, original, compressed):\n",
    "        \"\"\"Calculate compression statistics\"\"\"\n",
    "        original_size = len(original.encode('utf-8'))\n",
    "        compressed_size = len(compressed.encode('utf-8'))\n",
    "        ratio = original_size / compressed_size if compressed_size > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'original_size': original_size,\n",
    "            'compressed_size': compressed_size,\n",
    "            'compression_ratio': ratio,\n",
    "            'space_saving': (1 - 1/ratio) * 100 if ratio > 0 else 0\n",
    "        }\n",
    "\n",
    "def create_deep_corpus(output_file=\"deep_corpus.txt\", corpus_size=100000000000000000):\n",
    "    \"\"\"\n",
    "    Create a deep English corpus by downloading classic literature texts\n",
    "    from Project Gutenberg. This creates a rich training set for compression.\n",
    "    \n",
    "    Args:\n",
    "        output_file: File to save the corpus to\n",
    "        corpus_size: Approximate size of corpus in characters\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created corpus file\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of classic literature texts from Project Gutenberg (public domain)\n",
    "    gutenberg_texts = [\n",
    "    # Classic Literature - Fiction\n",
    "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride and Prejudice by Jane Austen\n",
    "    \"https://www.gutenberg.org/files/84/84-0.txt\",      # Frankenstein by Mary Shelley\n",
    "    \"https://www.gutenberg.org/files/98/98-0.txt\",      # A Tale of Two Cities by Charles Dickens\n",
    "    \"https://www.gutenberg.org/files/2701/2701-0.txt\",  # Moby Dick by Herman Melville\n",
    "    \"https://www.gutenberg.org/files/345/345-0.txt\",    # Dracula by Bram Stoker\n",
    "    \"https://www.gutenberg.org/files/76/76-0.txt\",      # Adventures of Huckleberry Finn by Mark Twain\n",
    "    \"https://www.gutenberg.org/files/1661/1661-0.txt\",  # Sherlock Holmes by Arthur Conan Doyle\n",
    "    \"https://www.gutenberg.org/files/174/174-0.txt\",    # The Picture of Dorian Gray by Oscar Wilde\n",
    "    \"https://www.gutenberg.org/files/145/145-0.txt\",    # Middlemarch by George Eliot\n",
    "    \n",
    "    # Classic Literature - Additional Genres\n",
    "    \"https://www.gutenberg.org/files/2641/2641-0.txt\",  # A Room with a View by E.M. Forster\n",
    "    \"https://www.gutenberg.org/files/11/11-0.txt\",      # Alice's Adventures in Wonderland by Lewis Carroll\n",
    "    \"https://www.gutenberg.org/files/1400/1400-0.txt\",  # Great Expectations by Charles Dickens\n",
    "    \"https://www.gutenberg.org/files/768/768-0.txt\",    # Wuthering Heights by Emily Brontë\n",
    "    \"https://www.gutenberg.org/files/2814/2814-0.txt\",  # Dubliners by James Joyce\n",
    "    \n",
    "    # Science and Philosophy\n",
    "    \"https://www.gutenberg.org/files/1228/1228-0.txt\",  # On the Origin of Species by Charles Darwin\n",
    "    \"https://www.gutenberg.org/files/5001/5001-0.txt\",  # Critique of Pure Reason by Immanuel Kant\n",
    "    \"https://www.gutenberg.org/files/4583/4583-0.txt\",  # The Theory of Relativity by Albert Einstein\n",
    "    \"https://www.gutenberg.org/files/28233/28233-0.txt\", # Mathematical Principles of Natural Philosophy by Newton\n",
    "    \"https://www.gutenberg.org/files/33283/33283-0.txt\", # Opticks by Isaac Newton\n",
    "    \"https://www.gutenberg.org/files/30142/30142-0.txt\", # Discourse on Method by René Descartes\n",
    "    \n",
    "    # Historical and Political Texts\n",
    "    \"https://www.gutenberg.org/files/1404/1404-0.txt\",  # Wealth of Nations by Adam Smith\n",
    "    \"https://www.gutenberg.org/files/30254/30254-0.txt\", # The Communist Manifesto by Marx and Engels\n",
    "    \"https://www.gutenberg.org/files/150/150-0.txt\",    # A Vindication of the Rights of Woman by Mary Wollstonecraft\n",
    "    \"https://www.gutenberg.org/files/61/61-0.txt\",      # The U.S. Constitution and Bill of Rights\n",
    "    \"https://www.gutenberg.org/files/100/100-0.txt\",    # The Complete Works of Abraham Lincoln\n",
    "    \"https://www.gutenberg.org/files/34901/34901-0.txt\", # Democracy in America by Alexis de Tocqueville\n",
    "    \n",
    "    # Speeches and Rhetoric\n",
    "    \"https://www.gutenberg.org/files/2130/2130-0.txt\",  # Washington's Farewell Address\n",
    "    \"https://www.gutenberg.org/files/16/16-0.txt\",      # The Declaration of Independence\n",
    "    \"https://www.gutenberg.org/files/35073/35073-0.txt\", # Gettysburg Address by Abraham Lincoln\n",
    "    \"https://www.gutenberg.org/files/49593/49593-0.txt\", # Winston Churchill's Speeches\n",
    "    \n",
    "    # Technical and Scientific\n",
    "    \"https://www.gutenberg.org/files/37134/37134-0.txt\", # The Principles of Psychology by William James\n",
    "    \"https://www.gutenberg.org/files/14988/14988-0.txt\", # Elements of Chemistry by Antoine Lavoisier\n",
    "    \"https://www.gutenberg.org/files/10662/10662-0.txt\", # The Measurement of Intelligence by Lewis Terman\n",
    "    \"https://www.gutenberg.org/files/5000/5000-0.txt\",  # The Principles of Mathematics by Bertrand Russell\n",
    "    \"https://www.gutenberg.org/files/5740/5740-0.txt\",  # Relativity: The Special and General Theory by Einstein\n",
    "    \n",
    "    # Essays and Non-Fiction\n",
    "    \"https://www.gutenberg.org/files/10615/10615-0.txt\", # Essays by Ralph Waldo Emerson\n",
    "    \"https://www.gutenberg.org/files/1080/1080-0.txt\",  # Walden by Henry David Thoreau\n",
    "    \"https://www.gutenberg.org/files/74/74-0.txt\",      # The Adventures of Tom Sawyer by Mark Twain\n",
    "    \"https://www.gutenberg.org/files/4705/4705-0.txt\",  # Essays of Francis Bacon\n",
    "    \"https://www.gutenberg.org/files/200/200-0.txt\",    # Sense and Sensibility by Jane Austen\n",
    "    \n",
    "    # Various Subjects\n",
    "    \"https://www.gutenberg.org/files/2680/2680-0.txt\",  # Meditations by Marcus Aurelius\n",
    "    \"https://www.gutenberg.org/files/3300/3300-0.txt\",  # An Enquiry Concerning Human Understanding by David Hume\n",
    "    \"https://www.gutenberg.org/files/41/41-0.txt\",      # The Strange Case of Dr. Jekyll and Mr. Hyde by R.L. Stevenson\n",
    "    \"https://www.gutenberg.org/files/2600/2600-0.txt\",  # War and Peace by Leo Tolstoy\n",
    "    \"https://www.gutenberg.org/files/158/158-0.txt\",    # Emma by Jane Austen\n",
    "]\n",
    "    \n",
    "    # Create corpus by downloading and concatenating texts\n",
    "    corpus_text = \"\"\n",
    "    texts_used = 0\n",
    "    \n",
    "    for url in gutenberg_texts:\n",
    "        try:\n",
    "            print(f\"Downloading {url}\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                # Extract the text, removing Project Gutenberg header/footer\n",
    "                text = response.text\n",
    "                \n",
    "                # Remove Gutenberg header (everything before the first empty line after \"START\")\n",
    "                start_marker = \"*** START OF\"\n",
    "                if start_marker in text:\n",
    "                    start_pos = text.find(start_marker)\n",
    "                    text = text[start_pos:]\n",
    "                    # Find the first paragraph break after the marker\n",
    "                    para_break = text.find(\"\\r\\n\\r\\n\")\n",
    "                    if para_break > 0:\n",
    "                        text = text[para_break+4:]\n",
    "                \n",
    "                # Remove Gutenberg footer (everything after \"END\")\n",
    "                end_marker = \"*** END OF\"\n",
    "                if end_marker in text:\n",
    "                    end_pos = text.find(end_marker)\n",
    "                    text = text[:end_pos]\n",
    "                \n",
    "                # Add to corpus\n",
    "                corpus_text += text + \"\\n\\n\"\n",
    "                texts_used += 1\n",
    "                \n",
    "                # Check if we've reached the target size\n",
    "                if len(corpus_text) >= corpus_size:\n",
    "                    break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "    \n",
    "    # Save corpus to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(corpus_text)\n",
    "    \n",
    "    print(f\"Created corpus with {len(corpus_text)} characters from {texts_used} texts\")\n",
    "    print(f\"Saved to {output_file}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def train_deep_compressor(corpus_path=None, output_path=\"deep_compressor.pkl\", max_patterns=512):\n",
    "    \"\"\"\n",
    "    Train a TextCompressor on a deep English corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus_path: Path to corpus file (will create one if None)\n",
    "        output_path: Where to save the trained compressor\n",
    "        max_patterns: Maximum number of patterns to include in the dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Trained TextCompressor instance\n",
    "    \"\"\"\n",
    "    # Create corpus if needed\n",
    "    if not corpus_path:\n",
    "        corpus_path = create_deep_corpus()\n",
    "    \n",
    "    print(f\"Training compressor on {corpus_path} with max_patterns={max_patterns}\")\n",
    "    \n",
    "    # Load corpus\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        corpus = f.read()\n",
    "    \n",
    "    # Create and train compressor\n",
    "    compressor = TextCompressor()\n",
    "    \n",
    "    # Train with a lower minimum frequency for more patterns but higher max_patterns\n",
    "    start_time = time.time()\n",
    "    compressor.train(corpus, min_frequency=3, max_patterns=max_patterns)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.1f} seconds\")\n",
    "    print(f\"Trained dictionary has {len(compressor.encoding_dict)} patterns\")\n",
    "    \n",
    "    # Save the trained compressor\n",
    "    compressor.save_dictionary(output_path)\n",
    "    print(f\"Saved compressor to {output_path}\")\n",
    "    \n",
    "    # Show some statistics about the trained patterns\n",
    "    patterns = list(compressor.encoding_dict.keys())\n",
    "    word_patterns = [p for p in patterns if re.match(r'^\\w+$', p)]\n",
    "    ngram_patterns = [p for p in patterns if not re.match(r'^\\w+$', p)]\n",
    "    \n",
    "    print(f\"\\nDictionary Statistics:\")\n",
    "    print(f\"- Words: {len(word_patterns)} ({len(word_patterns)/len(patterns)*100:.1f}%)\")\n",
    "    print(f\"- N-grams: {len(ngram_patterns)} ({len(ngram_patterns)/len(patterns)*100:.1f}%)\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nTop 10 longest words in dictionary:\")\n",
    "    top_words = sorted(word_patterns, key=len, reverse=True)[:10]\n",
    "    for word in top_words:\n",
    "        print(f\"  '{word}' -> '{repr(compressor.encoding_dict[word])}'\")\n",
    "    \n",
    "    print(\"\\nTop 10 longest n-grams in dictionary:\")\n",
    "    top_ngrams = sorted(ngram_patterns, key=len, reverse=True)[:10]\n",
    "    for ngram in top_ngrams:\n",
    "        print(f\"  '{ngram}' -> '{repr(compressor.encoding_dict[ngram])}'\")\n",
    "    \n",
    "    return compressor\n",
    "\n",
    "def test_compression_performance(compressor, text_samples=None):\n",
    "    \"\"\"\n",
    "    Test the compression performance of a trained compressor\n",
    "    \n",
    "    Args:\n",
    "        compressor: Trained TextCompressor instance\n",
    "        text_samples: List of text samples to test (will use defaults if None)\n",
    "        \n",
    "    Returns:\n",
    "        Average compression ratio\n",
    "    \"\"\"\n",
    "    if text_samples is None:\n",
    "        text_samples = [\n",
    "            # Short message\n",
    "            \"This is a short test message.\",\n",
    "            \n",
    "            # Medium message with common patterns\n",
    "            \"The quick brown fox jumps over the lazy dog. This message should compress well.\",\n",
    "            \n",
    "            # Long message with literary style\n",
    "            \"\"\"In the quiet hours of the morning, when the world is still wrapped in shadows \n",
    "            and the first hints of sunlight begin to stretch across the horizon, there is a rare \n",
    "            kind of peace that settles over everything. The air feels crisper, cleaner, and filled \n",
    "            with a quiet sense of possibility, as though the universe itself is holding its breath \n",
    "            before the start of a new day.\"\"\",\n",
    "            \n",
    "            # Technical message\n",
    "            \"\"\"The compression algorithm works by identifying frequent patterns in the text\n",
    "            and replacing them with shorter codes. This approach is particularly effective\n",
    "            for natural language text where certain words and phrases appear repeatedly.\"\"\"\n",
    "        ]\n",
    "    \n",
    "    total_ratio = 0.0\n",
    "    \n",
    "    for i, sample in enumerate(text_samples):\n",
    "        compressed = compressor.encode(sample)\n",
    "        stats = compressor.compression_stats(sample, compressed)\n",
    "        \n",
    "        # Print only the required information\n",
    "        print(f\"Original message: '{sample}'\")\n",
    "        print(f\"Compressed message: {repr(compressed)}\")\n",
    "        print(f\"Original: {stats['original_size']} bytes, Compressed: {stats['compressed_size']} bytes\")\n",
    "        print(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "        print()\n",
    "        \n",
    "        total_ratio += stats['compression_ratio']\n",
    "    \n",
    "    avg_ratio = total_ratio / len(text_samples)\n",
    "    return avg_ratio\n",
    "\n",
    "def send_encoded_message(message, compressor=None):\n",
    "    \"\"\"\n",
    "    Compress and send a message using ggwave\n",
    "    \n",
    "    Args:\n",
    "        message: Text message to compress and send\n",
    "        compressor: Optional TextCompressor instance (will create or load one if None)\n",
    "    \"\"\"\n",
    "    # Use provided compressor or load a pre-trained one\n",
    "    if compressor is None:\n",
    "        if os.path.exists(\"deep_compressor.pkl\"):\n",
    "            compressor = TextCompressor(\"deep_compressor.pkl\")\n",
    "        else:\n",
    "            compressor = train_deep_compressor()\n",
    "    \n",
    "    # Compress the message\n",
    "    original_message = message\n",
    "    compressed_message = compressor.encode(message)\n",
    "    \n",
    "    # Print only the required information\n",
    "    stats = compressor.compression_stats(original_message, compressed_message)\n",
    "    print(f\"Original message: '{original_message}'\")\n",
    "    print(f\"Compressed message: {repr(compressed_message)}\")\n",
    "    print(f\"Original: {stats['original_size']} bytes, Compressed: {stats['compressed_size']} bytes\")\n",
    "    print(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize ggwave\n",
    "        instance = ggwave.init()\n",
    "        \n",
    "        # Encode the message with ggwave - correct parameter order\n",
    "        waveform = ggwave.encode(compressed_message, instance)\n",
    "        \n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(suffix='.raw', delete=False) as temp_file:\n",
    "            temp_filename = temp_file.name\n",
    "            temp_file.write(waveform)\n",
    "            temp_file.flush()\n",
    "        \n",
    "        # Play the audio without additional output\n",
    "        subprocess.run([\n",
    "            'play',\n",
    "            '-t', 'f32',\n",
    "            '-r', '48000',\n",
    "            '-c', '1',\n",
    "            '-b', '32',\n",
    "            temp_filename\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        # Clean up\n",
    "        os.unlink(temp_filename)\n",
    "        \n",
    "        # Free ggwave\n",
    "        ggwave.free(instance)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return compressed_message\n",
    "\n",
    "def listen_for_deep_encoded_messages(duration=60):\n",
    "    \"\"\"\n",
    "    Listen for deeply encoded messages using the trained compressor\n",
    "    \n",
    "    Args:\n",
    "        duration: How long to listen in seconds\n",
    "    \"\"\"\n",
    "    # Load the deep compressor if available\n",
    "    if os.path.exists(\"deep_compressor.pkl\"):\n",
    "        compressor = TextCompressor(\"deep_compressor.pkl\")\n",
    "    else:\n",
    "        compressor = train_deep_compressor()\n",
    "    \n",
    "    # Define a simple listen function\n",
    "    def listen_for_encoded_messages(duration, compressor):\n",
    "        \"\"\"Simple implementation of the listen function\"\"\"\n",
    "        print(f\"Listening for encoded messages for {duration} seconds...\")\n",
    "        \n",
    "        # Initialize ggwave\n",
    "        instance = ggwave.init()\n",
    "        \n",
    "        # Track received messages\n",
    "        received_messages = set()\n",
    "        \n",
    "        try:\n",
    "            # Create a temporary file for recording\n",
    "            with tempfile.NamedTemporaryFile(suffix='.raw', delete=False) as temp_file:\n",
    "                temp_filename = temp_file.name\n",
    "            \n",
    "            # Start recording\n",
    "            sox_process = subprocess.Popen([\n",
    "                'rec',\n",
    "                '-t', 'f32',\n",
    "                '-r', '48000',\n",
    "                '-c', '1',\n",
    "                '-b', '32',\n",
    "                temp_filename\n",
    "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            \n",
    "            # Wait and check for messages\n",
    "            start_time = time.time()\n",
    "            end_time = start_time + duration\n",
    "            last_check_size = 0\n",
    "            \n",
    "            while time.time() < end_time:\n",
    "                time.sleep(1)\n",
    "                \n",
    "                try:\n",
    "                    # Check for new data\n",
    "                    current_size = os.path.getsize(temp_filename)\n",
    "                    \n",
    "                    if current_size > last_check_size:\n",
    "                        # Read new data\n",
    "                        with open(temp_filename, 'rb') as f:\n",
    "                            f.seek(last_check_size)\n",
    "                            new_data = f.read()\n",
    "                        \n",
    "                        last_check_size = current_size\n",
    "                        \n",
    "                        # Try to decode\n",
    "                        result = ggwave.decode(instance, new_data)\n",
    "                        \n",
    "                        if result is not None:\n",
    "                            message = result.decode('utf-8')\n",
    "                            \n",
    "                            # Check if compressed\n",
    "                            is_compressed = message and message[0] == compressor.COMPRESSION_MARKER\n",
    "                            \n",
    "                            if is_compressed:\n",
    "                                original = message\n",
    "                                message = compressor.decode(message)\n",
    "                                stats = compressor.compression_stats(message, original)\n",
    "                                \n",
    "                                if message not in received_messages:\n",
    "                                    print(f\"Original message: '{message}'\")\n",
    "                                    print(f\"Compressed message: {repr(original)}\")\n",
    "                                    print(f\"Original: {stats['original_size']} bytes, Compressed: {stats['compressed_size']} bytes\")\n",
    "                                    print(f\"Compression ratio: {stats['compression_ratio']:.2f}x\")\n",
    "                                    print()\n",
    "                                    received_messages.add(message)\n",
    "                            else:\n",
    "                                if message not in received_messages:\n",
    "                                    print(f\"Received uncompressed message: {message}\")\n",
    "                                    received_messages.add(message)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    \n",
    "        finally:\n",
    "            # Clean up\n",
    "            if 'sox_process' in locals():\n",
    "                sox_process.terminate()\n",
    "            \n",
    "            if 'temp_filename' in locals() and os.path.exists(temp_filename):\n",
    "                os.unlink(temp_filename)\n",
    "            \n",
    "            ggwave.free(instance)\n",
    "        \n",
    "        return list(received_messages)\n",
    "    \n",
    "    return listen_for_encoded_messages(duration, compressor)\n",
    "\n",
    "# Example usage in a notebook\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we already have a trained compressor\n",
    "        if os.path.exists(\"deep_compressor.pkl\"):\n",
    "            print(\"Loading existing compressor...\")\n",
    "            compressor = TextCompressor(\"deep_compressor.pkl\")\n",
    "        else:\n",
    "            # Create the corpus and train a deep compressor\n",
    "            print(\"Creating corpus and training compressor...\")\n",
    "            corpus_path = create_deep_corpus()\n",
    "            compressor = train_deep_compressor(corpus_path)\n",
    "\n",
    "        example_message = \"In an age where technology evolves at a breakneck pace, the integration of artificial intelligence into daily life has become not just a possibility, but an inevitability that touches nearly every industry and facet of human interaction, transforming the very way people live, work, learn, and relate to one another. From the automation of repetitive, mundane tasks to the development of highly sophisticated systems capable of learning, reasoning, interpreting context, and even producing original content, AI has become a cornerstone of the digital era. In healthcare, machine learning models are being used to predict patient outcomes, identify rare diseases, and streamline administrative tasks, reducing human error and enabling doctors to focus more on personalized care. In education, adaptive learning platforms can assess a student’s strengths and weaknesses in real time, tailoring the curriculum to maximize engagement and understanding while simultaneously providing educators with actionable insights. The financial industry leverages AI to detect fraudulent transactions in milliseconds, model complex market behaviors, and offer personalized investment strategies, while supply chain and logistics companies use predictive algorithms to optimize delivery routes, forecast demand, and reduce costs. Meanwhile, autonomous vehicles, once the stuff of science fiction, are being tested and deployed in real-world environments, navigating complex traffic scenarios with increasing confidence and safety. Even in creative fields like music, writing, and visual arts, AI tools are collaborating with human artists to generate entirely new forms of expression, challenging traditional definitions of creativity and authorship. Smart assistants such as Siri, Alexa, and Google Assistant have become ubiquitous in homes, capable of handling tasks from managing calendars to controlling smart home devices, while large language models like ChatGPT are being utilized in customer support, coding assistance, legal analysis, and mental health applications. Despite these incredible advancements, however, the rise of AI has sparked urgent debates around issues like data privacy, surveillance, algorithmic bias, job displacement, and the potential misuse of autonomous systems. There are growing concerns about the opacity of complex neural networks and the challenge of holding algorithms accountable for decisions that may carry ethical or legal implications. As AI systems are trained on vast datasets, often scraped from the internet, they may inadvertently perpetuate harmful stereotypes or reinforce existing societal inequalities. Furthermore, as automation continues to replace certain job functions, there is a critical need to rethink education, workforce training, and economic policy to ensure that people are equipped to thrive in a world where human and machine collaboration is the norm. Governments, corporations, and communities must work together to establish transparent guidelines and frameworks that prioritize fairness, accountability, and inclusivity in the development and deployment of AI technologies. In doing so, society has the opportunity to shape a future in which artificial intelligence not only enhances productivity and innovation but also serves as a tool for promoting equity, creativity, and human well-being across the globe. The path forward is complex and multifaceted, but with thoughtful design, robust regulation, and a commitment to ethical principles, the vast potential of AI can be harnessed to solve some of humanity’s most pressing challenges while enriching the collective human experience in profound and transformative ways.\"\n",
    "        \n",
    "        # Send the message using the deep compressor\n",
    "        listen_for_deep_encoded_messages()\n",
    "    except Exception as e: \n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci527",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
